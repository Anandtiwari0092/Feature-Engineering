{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?"
      ],
      "metadata": {
        "id": "RwzQJmArw0bE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lYsHBVzvfVf"
      },
      "outputs": [],
      "source": [
        "# A parameter is something the model learns automatically from the data to understand the patterns and relationships in it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation ? What does negative correlation mean?"
      ],
      "metadata": {
        "id": "XhQ8lSqmxCfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - Correlationrefers to a statistical relationship between two variables. It measures the strength and direction of how one variable changes in relation to another.\n",
        "# Negative correlation means that as one variable increases, the other variable decreases, and vice versa."
      ],
      "metadata": {
        "id": "hUiTbMVDxU0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "KfULU3rvxaSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - machine learning is a field in data science that focus in building system that can learn from data and improve their performance on a task over time.\n",
        "# The main component of machine learning are data, models, algorithms, and evaluation metrics."
      ],
      "metadata": {
        "id": "tmWs-gLuygwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "ocJSRnhV0A4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - The loss value is a key indicator of how well a machine learning model is performing.\n",
        "# It measures the difference between the model’s predictions and the actual target values.\n",
        "# The loss value tells how far off your model's predictions are from the actual results.A small loss means the model is doing a good job.\n",
        "# Monitoring and minimizing the loss is essential for training accurate and reliable machine learning models."
      ],
      "metadata": {
        "id": "BuMhYmlT0FK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "raJWqf_40iyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuous Variables:\n",
        "# Continuous variables are numeric values that can take an infinite number of values within a range.\n",
        "# They are measurable and often involve decimal points.\n",
        "\n",
        "# Categorical Variables:\n",
        "# Categorical variables represent groups or categories.\n",
        "# They describe qualitative data, not measured but classified.\n",
        "# Values are labels or names, not numbers"
      ],
      "metadata": {
        "id": "CH2kUo9a0ptz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
      ],
      "metadata": {
        "id": "M_TEg8sN2ZyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - Categorical variables cannot be used directly in most machine learning algorithms because they contain non-numeric data.\n",
        "# So, we must convert them into numerical formats while preserving their meaning."
      ],
      "metadata": {
        "id": "lC-xchfP2jfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "ZYDjdbfL8EcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a Dataset:\n",
        "#  Training means using a portion of the dataset to teach the machine learning model to recognize patterns.\n",
        "#  During training, the model learns the relationship between input features and the target output.\n",
        "#  The model adjusts its parameters (like weights in neural networks) to minimize errors on this data.\n",
        "\n",
        "# Testing a Dataset:\n",
        "#  Testing means evaluating the model’s performance on a different portion of data that it has never seen before.\n",
        "#  This helps check if the model can generalize well to new, unseen data and not just memorize the training examples.\n",
        "#  The test data acts as a real-world check on how accurate or reliable the model is.\n"
      ],
      "metadata": {
        "id": "0yFUestq8Ih_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "mkv0-1PR8ooa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - sklearn.preprocessing is a module in the scikit-learn librarythat provides tools and functions to preprocess and transform data before feeding it into machine learning models.\n"
      ],
      "metadata": {
        "id": "CXZgReJx8ump"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?"
      ],
      "metadata": {
        "id": "yTxFXgsn88tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - A Test Set is a portion of your dataset that is kept separate and not used during the training of a machine learning model.\n",
        "# It is used after training to evaluate how well the model performs on new, unseen data.\n"
      ],
      "metadata": {
        "id": "N6GRSvQ29A2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "F8DbMEO99Idt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To split data for training and testing in Python, we typically use the train_test_split() function from the sklearn.model_selection module.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Suppose X = features, y = target labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                     test_size=0.2,    # 20% data for testing\n",
        "                                                     random_state=42)  # ensures reproducible results\n"
      ],
      "metadata": {
        "id": "Qv_qUVa29dH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "Z9ayCq_jKpKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections.abc import ValuesView\n",
        "# Ans - EDA (Exploratory Data Analysis) is the process of analyzing and visualizing data to understand its structure, patterns, and problems before building any machine learning model.\n",
        "# Main Reasons to Perform EDA Before Modeling:\n",
        "#  is to understand data structure.\n",
        "#  to detect missing Values\n",
        "\n"
      ],
      "metadata": {
        "id": "w_1n2nyOK6bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?"
      ],
      "metadata": {
        "id": "nlPrugfJLu3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - same as question no. 2"
      ],
      "metadata": {
        "id": "v_4PS--mLzxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?"
      ],
      "metadata": {
        "id": "S_pp_w5fL5et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - same as question no. 2"
      ],
      "metadata": {
        "id": "3bT52LKvMHvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "St_qtTu2MIpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - To find the correlation between variables in Python, we usually use the pandas library. Correlation tells us how strongly two variables are related — positively, negatively, or not at all.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# create a database\n",
        "# Example DataFrame\n",
        "data = {\n",
        "    'Age': [25, 30, 35, 40, 45],\n",
        "    'Salary': [25000, 30000, 35000, 40000, 45000],\n",
        "    'Experience': [1, 3, 5, 7, 9]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Use corr() to Find Correlation\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3jS04-5kMdrm",
        "outputId": "42b18daf-7c7b-481a-960c-203d7c137392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Age  Salary  Experience\n",
            "Age         1.0     1.0         1.0\n",
            "Salary      1.0     1.0         1.0\n",
            "Experience  1.0     1.0         1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example"
      ],
      "metadata": {
        "id": "gYGUNTMoM2jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - Correlation: A relationship where two variables move together\n",
        "#                   May not indicate which variable influences the other\n",
        "#                   Ice cream sales and drowning deaths both increase in summer\n",
        "\n",
        "#        Causation: A direct cause-and-effect relationship\n",
        "#                   Clearly shows one variable causes the other\n",
        "#                   Smoking causes lung cancer\n"
      ],
      "metadata": {
        "id": "FZSjrWpwM9kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example"
      ],
      "metadata": {
        "id": "pKza3n6dNozy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - An optimizer is a key component in machine learning and deep learning that adjusts the model's parameters o minimize the loss function during training.\n",
        "# Its main job is to improve the model's performance by reducing errors between the predicted output and the actual target values.\n",
        "\n",
        "# here are different type of optimizer ;\n",
        "\n",
        "# 1. Gradient Descent (GD):\n",
        "#         It updates the weights by calculating the gradient of the loss function using the entire dataset.\n",
        "# 2. Stochastic Gradient Descent (SGD):\n",
        "#         Instead of the whole dataset, SGD updates the weights using only one data point at a time.\n",
        "# 3. Mini-Batch Gradient Descent:\n",
        "#         This is a combination of GD and SGD. It uses small batches (e.g., 32 or 64 samples) to compute gradients.\n",
        "# 4. Momentum Optimizer:\n",
        "#         This technique accelerates SGD by adding a fraction of the previous weight update, like momentum in physics.\n",
        "# 5. Nesterov Accelerated Gradient (NAG):\n",
        "#         An improved version of Momentum. It first moves the weights in the direction of momentum, then calculates the gradient.\n",
        "# 6. Adagrad:\n",
        "#         Adapts the learning rate for each parameter individually.\n",
        "# 7. RMSProp:\n",
        "#         Improves Adagrad by using a moving average of squared gradients.\n",
        "# 8. Adam (Adaptive Moment Estimation):\n",
        "#         Combines both Momentum and RMSProp.\n",
        "# 9. AdaMax:\n",
        "#         A variant of Adam that uses infinity norm instead of the L2 norm.\n",
        "# 10. Nadam:\n",
        "#         Combines Adam and Nesterov momentum.\n",
        "# 11. L-BFGS\n",
        "#         A second-order optimizer that uses approximations of the Hessian matrix.\n",
        "\n"
      ],
      "metadata": {
        "id": "5nx9ZL2vEgpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "4J60wPZxL0_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANS - sklearn.linear_model is a module in the Scikit-learn library (commonly imported as sklearn)\n",
        "# that provides classes and functions to perform linear models for regression and classification tasks in machine learning.\n",
        "# It contains different types of algorithms where the output is assumed to be a linear combination of the input features."
      ],
      "metadata": {
        "id": "xjKaf80dMUST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "VU9P2ZBvMpqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer:\n",
        "# The model.fit() function is used in Scikit-learn to train (or fit) a machine learning model on a given dataset. It is one of the most important steps in the machine learning pipeline."
      ],
      "metadata": {
        "id": "7wuYIIjQM60R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "rAjfaSCVNDoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model.predict() function is used in machine learning libraries (like scikit-learn, Keras, etc.) to make predictions on new data after a model has been trained."
      ],
      "metadata": {
        "id": "dDu4qLoROaLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "ou4HvJKfRKr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - same as question no. 5"
      ],
      "metadata": {
        "id": "TGaKJ4SiVB4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "ESpKTJkEVtcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - Feature Scaling is a data preprocessing technique used to normalize or standardize the range of independent variables (features) in a dataset.\n",
        "\n",
        "# Why is Feature Scaling Important in Machine Learning?\n",
        "# Some machine learning algorithms are sensitive to the magnitude (scale) of feature values. If one feature has a much larger range than others,\n",
        "#  it can dominate the learning process, leading to poor model performance."
      ],
      "metadata": {
        "id": "uXVSrhFOW0y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "DNEaH1sCXDn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - In Python, feature scaling is typically done using the scikit-learn library, which provides built-in tools to scale your data before feeding it into a machine learning model.\n",
        "# You choose a scaling technique (e.g., standardization or normalization) and import the appropriate scaler from sklearn.preprocessing.\n",
        "# This step calculates the required statistics (like mean, standard deviation, min, or max) from the training data.\n",
        "# Apply the scaling to your training and test data using the learned statistics.\n"
      ],
      "metadata": {
        "id": "to7DSZU6XXkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "n1VnSA_XYz0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - sklearn.preprocessing is a module in the scikit-learn library that provides tools for preprocessing and transforming data before feeding it into a machine learning model."
      ],
      "metadata": {
        "id": "Hd0yWky9ZbyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "VVxy-vikZv4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - same as question no. 10"
      ],
      "metadata": {
        "id": "vBzYMLK4Z8X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?"
      ],
      "metadata": {
        "id": "HN1pdYHfaLfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ans - Data encoding is the process of converting categorical (non-numeric) data into numeric form so that machine learning algorithms can understand and work with it.\n",
        "# Data encoding is crucial for preparing categorical features so machine learning models can interpret and learn from them.\n",
        "# Choosing the right encoding method depends on whether the categories are ordered or unordered."
      ],
      "metadata": {
        "id": "c2yii3C6ariU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}